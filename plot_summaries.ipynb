{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mm_functions as mmf\n",
    "import params as pr\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data location\n",
    "# prefix_dir = '/scratch2/raphaelr/processed_data/'\n",
    "# sigpref = '/scratch2/raphaelr/processed_data/significance_maps/'\n",
    "prefix_dir = '/Users/raphael/Documents/work/data/'\n",
    "sigpref = '/Users/raphael/Documents/work/data/significance_maps/'\n",
    "\n",
    "# Vectors for plotting\n",
    "lon_qvec = np.linspace(-pr.half_width,pr.half_width,pr.half_width*pr.eff_factor+1)\n",
    "lat_qvec = np.linspace(-pr.half_width,pr.half_width,pr.half_width*pr.eff_factor+1)\n",
    "# Vectors for plotting HR\n",
    "lon_qvec_hr = np.linspace(-pr.half_width,pr.half_width,pr.half_width*pr.res_factor+1)\n",
    "lat_qvec_hr = np.linspace(-pr.half_width,pr.half_width,pr.half_width*pr.res_factor+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_vec = np.arange(1,9+1)\n",
    "var_lists = [['precip','wind_mag'],['precip_mask','swh'],['pm10','t2m']]\n",
    "name_lists = [['Rain','Wind'],['Rain','Wave'],['pm10','Heat']]\n",
    "mm_list = [[0,1],[0,1,2],[0,1,3],[0,1,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1733075",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_vec = np.arange(1,9+1)\n",
    "var_lists = [['precip'],['wind_mag'],['swh'],['pm10']]\n",
    "name_lists = [['Rain'],['Wind'],['Wave'],['pm10']]\n",
    "mm_list = [[0],[0,1],[0,2],[0,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vv, var_list in enumerate(var_lists):\n",
    "\n",
    "    name_list = name_lists[vv]\n",
    "    \n",
    "    # Variables will always be treated in that order!\n",
    "    feat_list = ['WCB','cold_front','DI']\n",
    "    var_feat_list = var_list + feat_list\n",
    "\n",
    "    print(var_feat_list)\n",
    "    n_clus = len(cluster_vec)\n",
    "    n_mm = len(mm_list)\n",
    "    summary_var = np.zeros((n_clus,n_mm))\n",
    "\n",
    "    for ii, cluster_number in enumerate(cluster_vec):   \n",
    "        # Make MM composite\n",
    "        mm_comp_list, dens_list  = mmf.make_composite(var_feat_list,mm_list,prefix_dir,cluster_number,'plot')\n",
    "\n",
    "        # Define significance map only for the first input variable\n",
    "        analysis_name = \"-\".join(list(map(var_feat_list.__getitem__, mm_list[0])))\n",
    "        sigdir= sigpref+'C'+str(cluster_number)+'-'+analysis_name+'.npy'\n",
    "        sigmap = np.load(sigdir)\n",
    "\n",
    "        trunc = []\n",
    "        # Now loop through, producing the summary variables\n",
    "        for jj, mm_comp in enumerate(mm_comp_list):     \n",
    "\n",
    "            # Setting to zero wherever not significant\n",
    "            mm_comp_list[jj][sigmap]=0\n",
    "\n",
    "            trunc.append(mm_comp_list[jj][40:121,40:121])\n",
    "#             trunc.append(mm_comp_list[jj])\n",
    "    \n",
    "            # Computing number of point to normalize by (should not change even if recomputed)\n",
    "            npts = (trunc[jj].shape[0]*trunc[jj].shape[1])\n",
    "\n",
    "            # Computing summary variable\n",
    "            summary_var[ii,jj] = np.sum(trunc[jj])/npts\n",
    "\n",
    "    sort_vec = np.argsort(summary_var[:,0])\n",
    "    sort_vec = sort_vec[::-1]\n",
    "    clus_label = [str(x) for x in (sort_vec+1)]\n",
    "\n",
    "\n",
    "    \n",
    "    if vv == 0: fig, axs = plt.subplots(len(var_lists),1)\n",
    "\n",
    "    ind = cluster_vec\n",
    "    ax = axs[vv]\n",
    "    ax.bar(ind, 100*summary_var[sort_vec,0], width=0.8,facecolor='white',edgecolor='k')\n",
    "    ax.bar(ind-0.8/3, 100*summary_var[sort_vec,1],width=0.8/3,color='black')\n",
    "    ax.bar(ind, 100*summary_var[sort_vec,2],width=0.8/3,color=(0.4, 0.4, 0.4))\n",
    "    ax.bar(ind+0.8/3, 100*summary_var[sort_vec,3],width=0.8/3,color=(0.7, 0.7, 0.7))\n",
    "    ax.set_xticks(ind, labels=clus_label)\n",
    "    ax.set_ylabel('Freq. of occurrence [%]')\n",
    "    atitle = \"-\".join(list(map(name_list.__getitem__, mm_list[0])))\n",
    "    ax.set_title(atitle+' and Dyn. Features')\n",
    "    if vv == len(var_lists)-1: ax.set_xlabel('Cluster ID')\n",
    "    if vv == 1: ax.legend(['Hazard','Hazard-WCB','Hazard-CF','Hazard-DI'])\n",
    "    \n",
    "plt.subplots_adjust(wspace = 0.3, hspace=0.35, left=0.15)\n",
    "\n",
    "fig.set_size_inches(5,8/3*len(var_lists))\n",
    "# fig.savefig('Features_compound.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables will always be treated in that order!\n",
    "mm_list = [[0],[1],[0,1],[0,1,2]]\n",
    "\n",
    "n_clus = len(cluster_vec)\n",
    "n_mm = len(mm_list)\n",
    "summary_var = np.zeros((n_clus,n_mm+2))\n",
    "for ii, cluster_number in enumerate(cluster_vec):   \n",
    "    # Make MM composite\n",
    "    mm_comp_list, dens_list  = mmf.make_composite(var_list,mm_list,prefix_dir,cluster_number,'plot')\n",
    "    \n",
    "    # Load significance maps for all variables\n",
    "    sigmap_part = []    \n",
    "    for gg, mm_comp in enumerate(mm_comp_list):     \n",
    "        # Name of the analysis and loading of the significance map\n",
    "        analysis_name = \"-\".join(list(map(var_list.__getitem__, mm_list[gg])))\n",
    "        sigdir= sigpref+'C'+str(cluster_number)+'-'+analysis_name+'.npy'\n",
    "        sigmap_part.append(np.load(sigdir))\n",
    "     \n",
    "    # Redefine, including the expanded definition \n",
    "    # This is absolutely beautiful!\n",
    "    sigmap = []\n",
    "    sigmap.append(~np.logical_or(~sigmap_part[0],~sigmap_part[2])) # [0]\n",
    "    sigmap.append(~np.logical_or(~sigmap_part[1],~sigmap_part[2])) # [1]\n",
    "    sigmap.append(sigmap_part[2]) # [2]\n",
    "    \n",
    "    trunc = []\n",
    "    # Now loop through, producing the summary variables\n",
    "    for jj, mm_comp in enumerate(mm_comp_list):     \n",
    "        \n",
    "        # Setting to zero wherever not significant\n",
    "        mm_comp_list[jj][sigmap[jj]]=0\n",
    "        \n",
    "        trunc.append(mm_comp_list[jj][40:121,40:121])\n",
    "        \n",
    "        # Computing number of point to normalize by (should not change even if recomputed)\n",
    "        npts = (trunc[jj].shape[0]*trunc[jj].shape[1])\n",
    "        \n",
    "        # Computing summary variable\n",
    "        summary_var[ii,jj] = np.sum(trunc[jj])/npts\n",
    "        \n",
    "    # Just an additional test - may be more than 100% because compound may be significant even when individual hazards are not...\n",
    "    # May be fixed by adding the individual variable sig. region AND the compound variable sig. region.\n",
    "    tmp = np.minimum(trunc[0],trunc[1])\n",
    "    summary_var[ii,-2] = np.sum(tmp)/npts\n",
    "    \n",
    "    # Annnnd just one more\n",
    "    sum_list = []\n",
    "    for hh, var_name in enumerate(var_list):\n",
    "        fn = prefix_dir+var_name+'/'+var_name+'_hr_bool_c'+str(cluster_number)+'.npz'\n",
    "        tmp = np.load(fn)\n",
    "        tmp_comp = tmp['comp']\n",
    "        sigdir= sigpref+'C'+str(cluster_number)+'-'+var_name+'.npy'\n",
    "        sigarray2D = sigmap[hh]\n",
    "        sigarray3D = np.repeat(np.expand_dims(sigarray2D, 2),tmp_comp.shape[2], axis=2)\n",
    "        tmp_comp[sigarray3D] = 0\n",
    "        \n",
    "        tmp_trunc = tmp_comp[40:121,40:121,:]\n",
    "        \n",
    "#         densarray3D = np.repeat(np.expand_dims(dens_list[hh]/tmp_comp.shape[2], 2),tmp_comp.shape[2], axis=2)\n",
    "        densarray3D = np.expand_dims(dens_list[hh]/tmp_comp.shape[2], 2)\n",
    "        \n",
    "        dens_trunc = densarray3D[40:121,40:121,:]\n",
    "        \n",
    "        # Normalizing variable\n",
    "#         norm_var[hh] = np.sum(dens_list[hh][~sigmap[hh]])/np.sum(~sigmap[hh])/tmp_comp.shape[2]\n",
    "        \n",
    "        sum_list.append(np.nansum(tmp_trunc/dens_trunc,axis=(0,1)))\n",
    "    # Select the shortest length to compare arrays\n",
    "    len1 = len(sum_list[0])\n",
    "    len2 = len(sum_list[1])\n",
    "    eff_len = np.minimum(len1,len2)    \n",
    "    min_vect = np.minimum(sum_list[0][-eff_len:],sum_list[1][-eff_len:]) \n",
    "    summary_var[ii,-1] = np.mean(min_vect)/npts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_vec = np.argsort(summary_var[:,2])\n",
    "sort_vec = sort_vec[::-1]\n",
    "clus_label = [str(x) for x in (sort_vec+1)]\n",
    "\n",
    "# 0 is precipitation occurrence, 1 is wind occurrence 2 is actual compound occurrence\n",
    "# 3 is occurrence if temporal (but not spatial) correpondence was perfect\n",
    "# 4 is occurrence if spatial (but not temporal) correspondence was perfect\n",
    "# This variable is if both temporal and spatial compounding were perfect\n",
    "perfect_comp = np.minimum(summary_var[sort_vec,0],summary_var[sort_vec,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1768c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted frequency\n",
    "w_vect = np.array([0.16, 0.12, 0.07, 0.2, 0.08, 0.09, 0.11, 0.09, 0.09])\n",
    "frac_vect = summary_var[sort_vec,2]*w_vect[sort_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a55695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(3,1)\n",
    "ind = cluster_vec\n",
    "ax = axs[0]\n",
    "ax.bar(ind, 100*summary_var[sort_vec,2], width=0.8,color='purple')\n",
    "ax.set_xticks(ind, labels=clus_label)\n",
    "ax.set_ylabel('Freq. of occurrence [%]')\n",
    "ax.set_title('Compound event')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.bar(ind, 100*summary_var[sort_vec,0], width=0.8,color='blue')\n",
    "ax.set_xticks(ind, labels=clus_label)\n",
    "ax.set_ylabel('Freq. of occurrence [%]')\n",
    "ax.set_title(name_list[0]+' event')\n",
    "\n",
    "ax = axs[2]\n",
    "ax.bar(ind, 100*summary_var[sort_vec,1], width=0.8,color='red')\n",
    "ax.set_xticks(ind, labels=clus_label)\n",
    "ax.set_xlabel('Cluster ID')\n",
    "ax.set_ylabel('Freq. of occurrence [%]')\n",
    "ax.set_title(name_list[1]+' event')\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.3, hspace=0.35, left=0.15)\n",
    "fig.set_size_inches(5,8)\n",
    "fig.savefig('Compound_diagnostics_'+name_list[0]+'_'+name_list[1]+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f473f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,1)\n",
    "ind = cluster_vec\n",
    "\n",
    "ax=axs[0]\n",
    "ax.bar(ind, 100*perfect_comp, width=0.8,color='grey')\n",
    "ax.bar(ind, 100*summary_var[sort_vec,2], width=0.8, color='purple')\n",
    "ax.set_xticks(ind, labels=clus_label)\n",
    "ax.set_ylabel('Freq. of occurrence [%]')\n",
    "ax.set_title('Ideal compound scenario')\n",
    "\n",
    "ax=axs[1]\n",
    "ax.bar(ind, 100*summary_var[sort_vec,2]/summary_var[sort_vec,3], width=0.8,color='grey')\n",
    "ax.set_xlim(0,10)\n",
    "ax.set_xticks(ind, labels=clus_label)\n",
    "ax.set_ylabel('Simultaneity [%]')\n",
    "ax.set_title('Simultaneity of '+name_list[0]+' and '+name_list[1]+' events')\n",
    "\n",
    "ax=axs[2]\n",
    "ax.bar(ind, 100*summary_var[sort_vec,2]/summary_var[sort_vec,4], width=0.8,color='grey')\n",
    "ax.set_xlim(0,10)\n",
    "ax.set_xticks(ind, labels=clus_label)\n",
    "ax.set_xlabel('Cluster ID')\n",
    "ax.set_ylabel('Overlap [%]')\n",
    "ax.set_title('Spatial overlap of '+name_list[0]+' and '+name_list[1]+' events')\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.3, hspace=0.35, left=0.15)\n",
    "fig.set_size_inches(5,8)\n",
    "fig.savefig('Compound_ideal_'+name_list[0]+'_'+name_list[1]+'.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
